{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to timm-pretrained model, then turn into series of unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.configs import HookedViTConfig\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "import timm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 12, 'd_model': 768, 'd_head': 64, 'model_name': 'timm/vit_base_patch32_224.augreg_in21k_ft_in1k', 'n_heads': 12, 'd_mlp': 3072, 'activation_name': 'gelu', 'eps': 1e-06, 'original_architecture': 'vit_base_patch32_224', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 32, 'image_size': 224, 'n_classes': 1000, 'n_params': 88224232, 'return_type': 'class_logits'}\n",
      "Loaded pretrained model vit_base_patch32_224 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "prisma_model = HookedViT.from_pretrained(\"vit_base_patch32_224\", \n",
    "                                         center_writing_weights=False, \n",
    "                                         fold_ln=False, \n",
    "                                         fold_value_biases=False,\n",
    "                                         use_attn_scale=False,\n",
    "                                         use_attn_in=True,\n",
    ")\n",
    "# timm_model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model = timm.create_model('vit_base_patch32_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images to [-1, 1]\n",
    "    # Resize to 224 x 224\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "testset = datasets.CIFAR10(root='/home/mila/s/sonia.joseph/ViT-Planetarium/data/cifar10', train=False, download=False, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(testloader))\n",
    "output, cache = prisma_model.run_with_cache(image)\n",
    "\n",
    "# for key in cache.keys():\n",
    "#     print(key, cache[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.patch_embed.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "activations[0].shape\n",
    "\n",
    "assert torch.allclose(activations[0], cache['embed'][0])\n",
    "assert torch.all(activations[0] == cache['embed'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "for k in timm_model.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedViTConfig(n_layers=12, d_model=768, d_head=64, d_mlp=3072, model_name='timm/vit_base_patch32_224.augreg_in21k_ft_in1k', n_heads=12, activation_name='gelu', d_vocab=-1, eps=1e-06, use_attn_result=False, use_attn_scale=True, use_split_qkv_input=False, use_hook_mlp_in=False, use_attn_in=False, use_local_attn=False, original_architecture='vit_base_patch32_224', from_checkpoint=False, checkpoint_index=None, checkpoint_label_type=None, checkpoint_value=None, tokenizer_name=None, window_size=None, attn_types=None, init_mode='gpt2', normalization_type='LN', device='cpu', n_devices=1, attention_dir='bidirectional', attn_only=False, seed=None, initializer_range=0.02, init_weights=True, scale_attn_by_inverse_layer_idx=False, positional_embedding_type='standard', final_rms=False, d_vocab_out=-1, parallel_attn_mlp=False, rotary_dim=None, n_params=88224232, use_hook_tokens=False, gated_mlp=False, default_prepend_bos=True, dtype=torch.float32, tokenizer_prepends_bos=None, n_key_value_heads=None, post_embedding_ln=False, rotary_base=10000, trust_remote_code=False, rotary_adjacent_pairs=False, layer_norm_after=False, weight_type='he', cls_std=1e-06, pos_std=0.02, n_channels=3, patch_size=32, image_size=224, classification_type='cls', n_classes=1000, return_type='class_logits', log_dir='logs', use_wandb=True, wandb_team_name='perceptual-alignment', wandb_project_name=None, log_frequency=1, print_every=0, optimizer_name='AdamW', lr=0.0003, weight_decay=0.01, loss_fn_name='CrossEntropy', batch_size=512, warmup_steps=10, scheduler_step=200, scheduler_gamma=0.8, early_stopping=False, early_stopping_patience=2, num_epochs=50, parent_dir='', save_dir='Checkpoints', save_checkpoints=True, save_cp_frequency=5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prisma_model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.8371e-02,  1.8895e-01, -2.4998e-02,  ..., -1.1429e-02,\n",
       "          -2.6565e-02, -1.1025e-02],\n",
       "         [-5.2308e-02,  1.4553e+00,  2.3567e-03,  ...,  1.2978e-01,\n",
       "           1.3742e-02, -7.8761e-03],\n",
       "         [-4.4628e-01,  2.6859e+00, -5.5638e-02,  ...,  8.2297e-02,\n",
       "           9.2797e-02,  4.3837e-04],\n",
       "         ...,\n",
       "         [ 5.9315e-01,  4.6065e-01, -4.4948e-02,  ..., -3.0244e-02,\n",
       "          -4.2538e-02,  3.7644e-02],\n",
       "         [ 5.5119e-01,  1.6894e+00, -5.4843e-02,  ..., -6.5307e-02,\n",
       "           5.1351e-02,  1.0790e-02],\n",
       "         [ 4.2329e-01,  1.1742e-03, -9.6804e-03,  ..., -9.4712e-02,\n",
       "           1.9008e-02,  3.3273e-02]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_model.state_dict()['pos_embed']\n",
    "cache['pos_embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.pos_drop.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "\n",
    "assert torch.allclose(activations[0], cache['blocks.0.hook_resid_pre'], atol=1e-2), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n",
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import einops \n",
    "\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.blocks[0].norm1.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(activations[0].shape)\n",
    "print(cache['blocks.0.ln1.hook_normalized'].shape)\n",
    "\n",
    "# Assert equal to the first layer\n",
    "assert torch.allclose(activations[0], cache['blocks.0.ln1.hook_normalized'][0], atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "**Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's compare qkv weights\n",
    "# QKV = timm_model.blocks[0].attn.qkv.weight\n",
    "# W_Q, W_K, W_V = torch.tensor_split(QKV, 3, dim=0)\n",
    "# t_Q = einops.rearrange(W_Q, \"(i h) m->h m i\", h=12)\n",
    "# t_K = einops.rearrange(W_K, \"(i h) m->h m i\", h=12)\n",
    "# t_V = einops.rearrange(W_V, \"(i h) m->h m i\", h=12)\n",
    "\n",
    "# p_Q = prisma_model.blocks[0].attn.W_Q\n",
    "# p_K = prisma_model.blocks[0].attn.W_K\n",
    "# p_V = prisma_model.blocks[0].attn.W_V\n",
    "\n",
    "# assert torch.allclose(p_Q, t_Q, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(p_K, t_K, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(p_V, t_V, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "\n",
    "# # qkv bias\n",
    "# bias_QKV = timm_model.blocks[0].attn.qkv.bias\n",
    "\n",
    "# b_Q, b_K, b_V = torch.tensor_split(bias_QKV, 3, dim=0)\n",
    "\n",
    "# bt_Q = einops.rearrange(b_Q, \"(i h) -> h i\", h=12)\n",
    "# bt_K = einops.rearrange(b_K, \"(i h) -> h i\", h=12)\n",
    "# bt_V = einops.rearrange(b_V, \"(i h) -> h i\", h=12)\n",
    "\n",
    "# bp_Q = prisma_model.blocks[0].attn.b_Q\n",
    "# bp_K = prisma_model.blocks[0].attn.b_K\n",
    "# bp_V = prisma_model.blocks[0].attn.b_V\n",
    "\n",
    "# assert torch.allclose(bp_Q, bt_Q, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(bp_K, bt_K, atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "# assert torch.allclose(bp_V, bt_V, atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix_corner(matrix, rows=1, cols=1):\n",
    "    \"\"\"\n",
    "    Prints the top-left corner of a matrix (tensor) up to the specified number of rows and columns.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix (torch.Tensor): The matrix (tensor) from which to print the corner.\n",
    "    - rows (int): The number of rows to include in the printed corner. Default is 5.\n",
    "    - cols (int): The number of columns to include in the printed corner. Default is 5.\n",
    "    \"\"\"\n",
    "    # Ensure the matrix is a PyTorch tensor\n",
    "    if not isinstance(matrix, torch.Tensor):\n",
    "        print(\"The input is not a PyTorch tensor.\")\n",
    "        return\n",
    "\n",
    "    # Get the size of the matrix\n",
    "    num_rows, num_cols = matrix.shape[:2]\n",
    "\n",
    "    # Adjust rows and cols if the matrix is smaller than specified dimensions\n",
    "    rows_to_print = min(rows, num_rows)\n",
    "    cols_to_print = min(cols, num_cols)\n",
    "\n",
    "    # Slice the matrix to get the top-left corner\n",
    "    corner = matrix[:rows_to_print, :cols_to_print]\n",
    "\n",
    "    print(f\"Top-left corner ({rows_to_print}x{cols_to_print}):\\n{corner}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QKV matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timm output torch.Size([1, 50, 2304])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qkv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm output\u001b[39m\u001b[38;5;124m\"\u001b[39m, activations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# qkv = activations[0].reshape(-1, 197, 3, 12, 64).permute(2, 0, 3, 1, 4)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# qkv = activations[0].reshape(-1, 197, 3, 12, 64).permute(2, 0, 3, 1, 4)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43mqkv\u001b[49m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, qkv\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"prisma shape\", cache['blocks.0.attn.hook_qkv'].shape)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qkv' is not defined"
     ]
    }
   ],
   "source": [
    "# First layer\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.blocks[0].attn.qkv.register_forward_hook(hook_fn)\n",
    "\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(\"timm output\", activations[0].shape)\n",
    "# qkv = activations[0].reshape(-1, 197, 3, 12, 64).permute(2, 0, 3, 1, 4)\n",
    "# qkv = activations[0].reshape(-1, 197, 3, 12, 64).permute(2, 0, 3, 1, 4)\n",
    "q, k, v = qkv.unbind(0)\n",
    "\n",
    "print(\"timm shape\", qkv.shape)\n",
    "# print(\"prisma shape\", cache['blocks.0.attn.hook_qkv'].shape)\n",
    "\n",
    "print(\"prisma q shape\", cache['blocks.0.attn.hook_q'].shape)\n",
    "\n",
    "# assert torch.allclose(qkv, cache['blocks.0.attn.hook_qkv'], atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "assert torch.allclose(q, cache['blocks.0.attn.hook_q'], atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "assert torch.allclose(k, cache['blocks.0.attn.hook_k'], atol=1e-6), \"Activations differ more than the allowed tolerance\"\n",
    "assert torch.allclose(v, cache['blocks.0.attn.hook_v'], atol=1e-6), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_q = q * 64 ** -0.5\n",
    "timm_attn_scores = scaled_q @ k.transpose(-2,-1)\n",
    "\n",
    "print(\"timm attn scores\", timm_attn_scores.shape)\n",
    "print(\"prisma attn scores\", cache['blocks.0.attn.hook_attn_scores'].shape)\n",
    "\n",
    "assert torch.allclose(timm_attn_scores, cache['blocks.0.attn.hook_attn_scores'], atol=1e-4), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_attn_pattern = timm_attn_scores.softmax(dim=-1) \n",
    "\n",
    "assert torch.allclose(timm_attn_pattern, cache['blocks.0.attn.hook_pattern'], atol=1e-4), \"Activations differ more than the allowed tolerance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "activations = []\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = timm_model.blocks[0].attn.proj.register_forward_hook(hook_fn)\n",
    "timm_output = timm_model(image)\n",
    "hook_handle.remove()\n",
    "\n",
    "print(activations[0].shape)\n",
    "# print(cache['blocks.0.attn.hook_attn_out'].shape)\n",
    "\n",
    "\n",
    "assert torch.allclose(cache['blocks.0.attn.hook_result'], activations[0], atol=1e-3), \"Activations differ more than the allowed tolerance\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#currently only vit_base_patch16_224 supported (config loading issue)\n",
    "TOLERANCE = 1e-5\n",
    "\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "batch_size = 5\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "device = \"cuda\"\n",
    "\n",
    "hooked_model = HookedViT.from_pretrained(model_name)\n",
    "hooked_model.to(device)\n",
    "timm_model = timm.create_model(model_name, pretrained=True)\n",
    "timm_model.to(device)\n",
    "\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(1)\n",
    "    input_image = torch.rand((batch_size, channels, height, width)).to(device)\n",
    "\n",
    "assert torch.allclose(hooked_model(input_image), timm_model(input_image), atol=TOLERANCE), \"Model output diverges!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model(input_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model(input_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prisma_env",
   "language": "python",
   "name": "prisma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
